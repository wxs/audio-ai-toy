<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link rel="stylesheet" href="static/css/bootstrap.min.css">
    <style>
    button {
      font-size: 21pt;
      padding: 10px;
    }

    #top-container {
      border: 1px solid black;
      padding: 5px;
    }
    </style>
  </head>

  <div class="container">
    <div class="row">
      <div class="col-md-12">
  <h1>AI audio processing</h1>


  <p>In this workshop we're going to try using some neural networks created by
    various research organizations for working with music.


  <div id="top-container">

  <div id="step1">
    <h2>Step 1: Record Your Voice</h2>
    <p>Click the microphone to record a few seconds of audio. Sing a song!
    <!--<button id="start-record">Record Audio</button>-->
  </div>

  <div id="step2">
    <h2>Step 2: Pitch Estimation with CREPE</h2>
    <p>Listen to the output audio below! If you don't like it, click the microphone
      to try again. Otherwise, let's feed it to our first neural network!

    <p> Here we will use a network called
      <a href='https://github.com/marl/crepe'>CREPE</a> from NYU. This neural
      network from 2018 is state of the art for pitch estimation.
    <p>Click on the CREPE box below to estimate pitch
  </div>

  <div id="step3">
    <h2>Step 3: Violin Synthesis with DDSP</h2>
    <p>Next we'll take the <strong>output</strong> of CREPE (i.e.
      the pitch information) and feed it as the <strong>input</strong> to another neural network,
      that has learned to generate musical instrument sounds from pitch. This
      neural network is part of the <a href='https://magenta.tensorflow.org/ddsp'>
      DDSP</a> project from the Google Magenta team.
  </div>

  </div>

  <div id="middle-container">
    <object type="image/svg+xml" data="static/flowchart.svg" style="width:100%" id="flowchart"></object>
  </div>
  <div id="bottom-container" class="row">
    <div id="step1-out" class="col-sm-4" style="display:none;">
      <h3>Recorded Audio</h3>
      <audio id="player" controls></audio>
      <p><a id="download-input" download="input_audio.wav">Download</a>
    </div>
    <div id="step2-out" class="col-sm-4" style="display: none;">
      <h3>Estimated Pitch</h3>
      <div id="pitchgraph" style="width:100%; height: 200px;"></div>
    </div>
    <div id="step3-out" class="col-sm-4" style="display: none;">
      <h3>Generated Violin</h3>
      <p><audio id="player-ddsp" controls></audio>
      <p><a id="download-ddsp" download="violin_audio.wav">Download</a>
    </div>
  </div>
</div>


<script src="static/WavAudioEncoder.min.js"></script>
<script src="static/jquery-3.6.0.min.js"></script>
<script src="static/dygraph.js"></script>
<script>
const CLIP_LENGTH = 4000;
const player = document.getElementById('player');
//const downloadButton = document.getElementById('download')
let f = null; // flowchart


let recordedAudio, crepeData, ddspData;

function plotCREPEData(crepeData) {
  let g = new Dygraph(
    $("#pitchgraph")[0],
    crepeData.f0_hz.map((element, index) => {return [index, element]}),
    {
      labels: ['Time (ms)', 'Frequency (Hz)']
    }
  );
}

function startRecord() {
  navigator.mediaDevices.getUserMedia({ audio: true, video: false })
      .then(function(stream) {
        startRecordAnimation();
        const context = new AudioContext();
        const source = context.createMediaStreamSource(stream);
        const processor = context.createScriptProcessor(1024, 1, 1);
        const encoder = new WavAudioEncoder(44100, 1);

        source.connect(processor);
        processor.connect(context.destination);

        processor.onaudioprocess = function(e) {
          encoder.encode([e.inputBuffer.getChannelData(0)]);
        };

        setTimeout(function() {
          stopRecordAnimation();
          stream.getTracks().forEach(function(track) {
          if (track.readyState == 'live') {
                  track.stop();
              }
          });
          processor.disconnect();
          source.disconnect();
          let blob = encoder.finish();
          const blobURL = URL.createObjectURL(blob);
          player.src = blobURL;
          $("#download-input").prop('href',blobURL);
          recordedAudio = blob;
          $("#step1-out").show();
          renderStep2();
        }, CLIP_LENGTH);
      });
}

function sendToCrepe() {
  if (!recordedAudio) {
    return;
  }
  let fd = new FormData();
  fd.append('audio', recordedAudio)
  $.ajax({
    url: '/crepe',
    data: fd,
    processData: false,
    contentType: false,
    type: 'POST',
    success: function(data) {
      stopCrepeAnimation();
      crepeData = data;
      $("#step2-out").show();
      renderStep3();
      // Needs to happen *after* showing step 2 out because dygraph has trouble
      // otherwise
      plotCREPEData(data);
    },
    error: function(jqxhr, textStatus, errorThrown) {
      console.log("ERROR IN CREPE POST");
      console.log(jqxhr, textStatus, errorThrown);
      stopCrepeAnimation();
    }
  });
  startCrepeAnimation();
}

function sendToDDSP() {
  if (!crepeData) {
    return;
  }
  startDDSPAnimation();
  $.ajax({
    type: 'POST',
    url: '/violin',
    contentType: "application/json",
    data: JSON.stringify(crepeData),
    xhr:function() {
         var xhr = new XMLHttpRequest();
         xhr.responseType= 'blob'
         return xhr;
     },
    success: function(data) {
      stopDDSPAnimation();
      ddspData = data;
      //let blob = new Blob([data], {type: "audio/wav"});
      const blobURL = URL.createObjectURL(data);
      $("#player-ddsp").prop('src',blobURL);
      $("#download-ddsp").prop('href',blobURL);
      $("#step3-out").show();
    },
    error: function(jqxhr, textStatus, errorThrown) {
      console.log("ERROR IN VIOLIN POST");
      console.log(jqxhr, textStatus, errorThrown);
      stopDDSPAnimation();
    }

  });
}

function renderStep1() {
  $("#step1").show();
  $("#step2").hide();
  $("#step3").hide();
  $(f.querySelectorAll('#layer2, #layer2 .highlightable')).addClass("future");
  $(f.querySelectorAll('#layer3, #layer3 .highlightable')).addClass("future");
}
function renderStep2() {
  $("#step1").hide();
  $("#step2").show();
  $("#step3").hide();
  $(f.querySelectorAll('#layer2, #layer2 .highlightable')).removeClass("future");
  $(f.querySelectorAll('#layer3, #layer3 .highlightable')).addClass("future");
}
function renderStep3() {
  $("#step1").hide();
  $("#step2").hide();
  $("#step3").show();
  $(f.querySelectorAll('#layer2, #layer2 .highlightable')).removeClass("future");
  $(f.querySelectorAll('#layer3, #layer3 .highlightable')).removeClass("future");
}

let recordAnimationId = null;
let crepeAnimationId = null;
let ddspAnimationId = null;
let micElements;

function rgbString(r,g,b) {
  return "rgb(" + r + "," + g + "," + b + ")";
}

function startRecordAnimation() {
  let start;
  micElements = f.querySelectorAll('#microphone .highlightable');
  $(micElements).addClass('animating');
  function loop(timestamp) {
    let interp = (Math.sin(timestamp/200.0)/2 + 0.5);
    let r = 255 * interp + 237 * (1 - interp);
    let g = 237 * (1 - interp);
    let b = 237 * (1 - interp);
    let fill = rgbString(r,g,b);
    micElements.forEach((el) => {
      el.style.fill = fill;
    });
    recordAnimationId = requestAnimationFrame(loop);
  }
  recordAnimationId = requestAnimationFrame(loop);
}

function stopRecordAnimation() {
  micElements.forEach((el) => {
    el.style.fill = "#ececec"
  })
  $(micElements).removeClass('animating');
  cancelAnimationFrame(recordAnimationId);
}


function getRandomAnimator(elements, period) {
  let nextChange = 0;
  return function(timestamp) {
    if (timestamp > nextChange) {
      elements.forEach((el) => {
        let r = Math.round(Math.random()*255);
        let g = Math.round(Math.random()*255);
        let b = Math.round(Math.random()*255);
        el.style.fill = rgbString(r,g,b);
      });
      nextChange = timestamp + period;
    }
  };
}
function startCrepeAnimation() {
  let anim = getRandomAnimator(f.querySelectorAll('#layer2 circle'), 100);
  function loop(timestamp) {
    anim(timestamp)
    crepeAnimationId = requestAnimationFrame(loop);
  }
  crepeAnimationId = requestAnimationFrame(loop);
}

function stopCrepeAnimation() {
  cancelAnimationFrame(crepeAnimationId);
  f.querySelectorAll('#layer2 circle').forEach((el) => {
    el.style.fill = "#000000";
  });
}

function startDDSPAnimation() {
  let anim = getRandomAnimator(f.querySelectorAll('#layer3 circle'), 100);
  function loop(timestamp) {
    anim(timestamp)
    ddspAnimationId = requestAnimationFrame(loop);
  }
  ddspAnimationId = requestAnimationFrame(loop);
}

function stopDDSPAnimation() {
  cancelAnimationFrame(ddspAnimationId);
  f.querySelectorAll('#layer3 circle').forEach((el) => {
    el.style.fill = "#000000";
  });
}

$("#flowchart").on('load', function() {
  f = this.contentDocument;
  let step1 = f.getElementById('layer1');
  let step2 = f.getElementById('layer2');
  let step3 = f.getElementById('layer3');

  step1.querySelectorAll('.highlightable').forEach((el) => {el.addEventListener('click', startRecord)});
  step2.querySelectorAll('.highlightable').forEach((el) => {el.addEventListener('click', sendToCrepe)});
  step3.querySelectorAll('.highlightable').forEach((el) => {el.addEventListener('click', sendToDDSP)});
  renderStep1();
});


</script>

</html>
